{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Ensemble Learning | Assignment\n"
      ],
      "metadata": {
        "id": "sWJttWE_b_dH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1: What is Ensemble Learning in machine learning? Explain the key idea behind it"
      ],
      "metadata": {
        "id": "UaOYCJ1icFMo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ":- Ensemble Learning is a machine learning technique where multiple models are combined to make a final prediction.\n",
        "\n",
        "Key idea:\n",
        "Instead of relying on one model, ensemble learning combines several weak or base models so that their collective decision is more accurate, stable, and robust than any single model."
      ],
      "metadata": {
        "id": "e5HGRz7sch--"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2: What is the difference between Bagging and Boosting?"
      ],
      "metadata": {
        "id": "CjsQ1gPgctQB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ":- Bagging (Bootstrap Aggregating):\n",
        "Trains multiple models independently on different random subsets of data and averages their predictions. It mainly reduces variance.\n",
        "Example: Random Forest\n",
        "\n",
        "Boosting:\n",
        "Trains models sequentially, where each new model focuses on correcting the errors of the previous ones. It mainly reduces bias."
      ],
      "metadata": {
        "id": "tiZVKCGOtJe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3: What is bootstrap sampling and what role does it play in Bagging methods like Random Forest?"
      ],
      "metadata": {
        "id": "ymKOticFtQZG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ":- Role in Bagging / Random Forest:\n",
        "\n",
        "Each model (tree) is trained on a different bootstrap sample.\n",
        "\n",
        "This creates diversity among models, reducing overfitting.\n",
        "\n",
        "The final prediction is made by averaging (regression) or voting (classification), which improves accuracy and stability."
      ],
      "metadata": {
        "id": "hxNb51JttWGX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4: What are Out-of-Bag (OOB) samples and how is OOB score used to evaluate ensemble models?"
      ],
      "metadata": {
        "id": "VUHBreLwtoqg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ":- OOB score usage:\n",
        "\n",
        "Each model is tested on its own OOB samples.\n",
        "\n",
        "Predictions from all models where a data point is OOB are aggregated.\n",
        "\n",
        "The OOB score estimates model performance without a separate validation set, giving an unbiased accuracy estimate."
      ],
      "metadata": {
        "id": "vF2u4OmutvGP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5: Compare feature importance analysis in a single Decision Tree vs. a Random Forest.\n"
      ],
      "metadata": {
        "id": "LhKefCm3t_Yb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ":- Decision Tree:\n",
        "Feature importance is based on how much each feature reduces impurity in that single tree.\n",
        "It is unstable—small data changes can alter importance a lot.\n",
        "\n",
        "Random Forest:\n",
        "Feature importance is averaged across many trees.\n",
        "It is more reliable and stable, capturing overall feature influence and reducing bias from a single tree."
      ],
      "metadata": {
        "id": "yvku176suJPr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#6: Write a Python program to:                                                 ● Load the Breast Cancer dataset using sklearn.datasets.load_breast_cancer() ● Train a Random Forest Classifier ● Print the top 5 most important features based on feature importance scores. (Include your Python code and output in the code box below.)"
      ],
      "metadata": {
        "id": "88Q2k70JuR6I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import pandas as pd\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "feature_names = data.feature_names\n",
        "\n",
        "# Train Random Forest Classifier\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "rf.fit(X, y)\n",
        "\n",
        "# Get feature importance\n",
        "importances = rf.feature_importances_\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    \"Feature\": feature_names,\n",
        "    \"Importance\": importances\n",
        "})\n",
        "\n",
        "# Sort and select top 5 features\n",
        "top_5 = feature_importance_df.sort_values(by=\"Importance\", ascending=False).head(5)\n",
        "\n",
        "print(top_5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jTB7O6Ubu1Zl",
        "outputId": "43e0d443-1d63-41fc-c02c-fd5833b5d9f3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                 Feature  Importance\n",
            "23            worst area    0.139357\n",
            "27  worst concave points    0.132225\n",
            "7    mean concave points    0.107046\n",
            "20          worst radius    0.082848\n",
            "22       worst perimeter    0.080850\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#7: Write a Python program to: ● Train a Bagging Classifier using Decision Trees on the Iris dataset ● Evaluate its accuracy and compare with a single Decision Tree (Include your Python code and output in the code box below.)"
      ],
      "metadata": {
        "id": "0zry-NsbusN7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Iris dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Single Decision Tree\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "dt.fit(X_train, y_train)\n",
        "dt_preds = dt.predict(X_test)\n",
        "dt_accuracy = accuracy_score(y_test, dt_preds)\n",
        "\n",
        "# Bagging Classifier with Decision Trees\n",
        "bagging = BaggingClassifier(\n",
        "    estimator=DecisionTreeClassifier(),\n",
        "    n_estimators=50,\n",
        "    random_state=42\n",
        ")\n",
        "bagging.fit(X_train, y_train)\n",
        "bag_preds = bagging.predict(X_test)\n",
        "bag_accuracy = accuracy_score(y_test, bag_preds)\n",
        "\n",
        "print(\"Decision Tree Accuracy:\", dt_accuracy)\n",
        "print(\"Bagging Classifier Accuracy:\", bag_accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DRdRtdDkvECg",
        "outputId": "5383d636-4ee3-478f-a020-024293398fd8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Accuracy: 1.0\n",
            "Bagging Classifier Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#8: Write a Python program to: ● Train a Random Forest Classifier ● Tune hyperparameters max_depth and n_estimators using GridSearchCV ● Print the best parameters and final accuracy (Include your Python code and output in the code box below.)"
      ],
      "metadata": {
        "id": "mhwbUtUfvYtS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Random Forest model\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Hyperparameter grid\n",
        "param_grid = {\n",
        "    \"n_estimators\": [50, 100],\n",
        "    \"max_depth\": [None, 5, 10]\n",
        "}\n",
        "\n",
        "# GridSearchCV\n",
        "grid = GridSearchCV(rf, param_grid, cv=5, scoring=\"accuracy\")\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "# Best model\n",
        "best_model = grid.best_estimator_\n",
        "\n",
        "# Final evaluation\n",
        "y_pred = best_model.predict(X_test)\n",
        "final_accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Best Parameters:\", grid.best_params_)\n",
        "print(\"Final Accuracy:\", final_accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L_n1PFJqvPbg",
        "outputId": "c9626b44-0642-40dd-93a0-5af17665da14"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': None, 'n_estimators': 100}\n",
            "Final Accuracy: 0.9707602339181286\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9: Write a Python program to: ● Train a Bagging Regressor and a Random Forest Regressor on the California Housing dataset ● Compare their Mean Squared Errors (MSE) (Include your Python code and output in the code box below.)"
      ],
      "metadata": {
        "id": "vkBaDFUCv016"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load California Housing dataset\n",
        "data = fetch_california_housing()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Bagging Regressor\n",
        "bagging = BaggingRegressor(\n",
        "    estimator=DecisionTreeRegressor(),\n",
        "    n_estimators=50,\n",
        "    random_state=42\n",
        ")\n",
        "bagging.fit(X_train, y_train)\n",
        "bag_preds = bagging.predict(X_test)\n",
        "bag_mse = mean_squared_error(y_test, bag_preds)\n",
        "\n",
        "# Random Forest Regressor\n",
        "rf = RandomForestRegressor(\n",
        "    n_estimators=100,\n",
        "    random_state=42\n",
        ")\n",
        "rf.fit(X_train, y_train)\n",
        "rf_preds = rf.predict(X_test)\n",
        "rf_mse = mean_squared_error(y_test, rf_preds)\n",
        "\n",
        "print(\"Bagging Regressor MSE:\", bag_mse)\n",
        "print(\"Random Forest Regressor MSE:\", rf_mse)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AcFflVmlvrfU",
        "outputId": "f3ed07c9-1da8-4bb8-beee-800e57ee675c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging Regressor MSE: 0.25787382250585034\n",
            "Random Forest Regressor MSE: 0.25650512920799395\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 10: You are working as a data scientist at a financial institution to predict loan default. You have access to customer demographic and transaction history data. You decide to use ensemble techniques to increase model performance. Explain your step-by-step approach to: ● Choose between Bagging or Boosting ● Handle overfitting ● Select base models ● Evaluate performance using cross-validation ● Justify how ensemble learning improves decision-making in this real-world context.\n"
      ],
      "metadata": {
        "id": "dYHNg33pwQHg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ":-  1. Choosing between Bagging and Boosting\n",
        "\n",
        "If the dataset is large and noisy with high variance models → choose Bagging (e.g., Random Forest).\n",
        "\n",
        "If the dataset has complex patterns and bias is high → choose Boosting (e.g., XGBoost, AdaBoost).\n",
        "\n",
        "For loan default prediction, Boosting is often preferred because it focuses on hard-to-classify defaulters, improving recall.\n",
        "\n",
        "2. Handling Overfitting\n",
        "\n",
        "Use cross-validation to monitor generalization.\n",
        "\n",
        "Apply regularization (limit tree depth, learning rate in boosting).\n",
        "\n",
        "In Bagging, overfitting is reduced naturally by averaging multiple models.\n",
        "\n",
        "In Boosting, control overfitting using early stopping and shallow trees.\n",
        "\n",
        "3. Selecting Base Models\n",
        "\n",
        "Use Decision Trees as base learners:\n",
        "\n",
        "They capture non-linear relationships.\n",
        "\n",
        "Work well with mixed data (demographic + transaction).\n",
        "\n",
        "Use shallow trees (weak learners) for Boosting.\n",
        "\n",
        "Use full or moderately deep trees for Bagging.\n",
        "\n",
        "4. Evaluating Performance (Cross-Validation)\n",
        "\n",
        "Use k-fold cross-validation to ensure stability.\n",
        "\n",
        "Evaluate using:\n",
        "\n",
        "Accuracy\n",
        "\n",
        "Precision & Recall (important for defaults)\n",
        "\n",
        "ROC-AUC (best for imbalanced data)\n",
        "\n",
        "Compare ensemble results with a single model baseline.\n",
        "\n",
        "5. Why Ensemble Learning Improves Decision-Making\n",
        "\n",
        "Combines multiple models → more robust predictions.\n",
        "\n",
        "Reduces variance (Bagging) and bias (Boosting).\n",
        "\n",
        "Improves detection of high-risk customers, reducing financial loss.\n",
        "\n",
        "Leads to more reliable, fair, and data-driven credit decisions."
      ],
      "metadata": {
        "id": "u_Gtl2aewyot"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mtOSMJGiwHe-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}